{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newsAPI     =    0b1ea46e336d40e1abc48f51e01c253a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pymongo\n",
    "import sys\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "# from collections import defaultdict\n",
    "# from datetime import timedelta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "publish_date             datetime64[ns, UTC]\n",
      "headline_text                         object\n",
      "description                           object\n",
      "headline_sentiment                     int64\n",
      "description_sentiment                  int64\n",
      "emotion                                int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('testData.csv')\n",
    "\n",
    "# convert the publish_date column to datetime64\n",
    "df['publish_date'] = pd.to_datetime(df['publish_date'])\n",
    "\n",
    "# convert remaining cols to string\n",
    "df['headline_text'] = df['headline_text'].astype(str)\n",
    "df['description'] = df['description'].astype(str)\n",
    "\n",
    "\n",
    "headlines_df = df.copy()\n",
    "headlines_df['headline_sentiment'] = 0\n",
    "headlines_df['description_sentiment'] = 0\n",
    "headlines_df['emotion'] = 0\n",
    "# headlines_df['pos_sentiment'] = 0\n",
    "# headlines_df['neg_sentiment'] = 0\n",
    "\n",
    "print(headlines_df.dtypes)\n",
    "headlines_df.head()\n",
    "\n",
    "total_headlines = len(headlines_df)\n",
    "\n",
    "headlines = df['headline_text'].tolist()\n",
    "descriptions = df['description'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_emotions = ['joy', 'others', 'surprise',\n",
    "                  'sadness', 'fear', 'anger', 'disgust']\n",
    "\n",
    "\n",
    "emotions =  {\"joy\": 0, \"others\": 0, \"surprise\": 0, \n",
    "             \"sadness\": 0, \"fear\": 0, \"anger\": 0, \"disgust\": 0}\n",
    "\n",
    "\n",
    "keywords = {}\n",
    "\n",
    "# emotions = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# sentiments = defaultdict(lambda: defaultdict(int))\n",
    "sentiment_model = pipeline(\n",
    "    model=\"siebert/sentiment-roberta-large-english\")  # Use device 0 for GPU\n",
    "\n",
    "emotion_model = pipeline(\n",
    "    model=\"finiteautomata/bertweet-base-emotion-analysis\")\n",
    "\n",
    "keyword_ext_model = pipeline(\n",
    "    model=\"yanekyuk/bert-keyword-extractor\")\n",
    "\n",
    "keyword_senti_model = pipeline(\n",
    "    model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.9994750618934631}]\n"
     ]
    }
   ],
   "source": [
    "# temp = sentiment_model(\"this is so frustrating that i get to do something i love\")\n",
    "\n",
    "# print(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(batch_headlines):\n",
    "    batch_results = keyword_ext_model(batch_headlines)\n",
    "\n",
    "    # keywords = {}\n",
    "\n",
    "    # print(batch_results)\n",
    "\n",
    "    for result in batch_results:\n",
    "        # for keyword_info in result:\n",
    "        # print(result)\n",
    "        keyword = result['word']\n",
    "        # print(keyword)\n",
    "        # Filter words with less than 2 letters, exclude hashtags, and exclude \"chin\"\n",
    "        if keyword and len(keyword) >= 3 and not keyword.startswith('#') and keyword.lower() != 'chin':\n",
    "            if keyword in keywords.keys():\n",
    "                keywords[keyword] += 1\n",
    "            else:\n",
    "                keywords[keyword] = 1\n",
    "\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analysing Sentiments:   0%|          | 0/5 [00:00<?, ?headline/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analysing Sentiments: 100%|██████████| 5/5 [00:02<00:00,  1.99headline/s]\n"
     ]
    }
   ],
   "source": [
    "## analyse headlines ##\n",
    "with tqdm(total=total_headlines, desc=\"Analysing Sentiments\", unit=\"headline\", dynamic_ncols=True) as pbar:\n",
    "    for idx in range(total_headlines):\n",
    "        row = headlines_df.iloc[idx]\n",
    "        headline = row['headline_text']\n",
    "        description = row['description']\n",
    "\n",
    "\n",
    "        result = sentiment_model(headline)\n",
    "        label = result[0]['label']\n",
    "\n",
    "        if label == 'POSITIVE':\n",
    "            headlines_df.at[idx, 'headline_sentiment'] = 1\n",
    "        elif label == 'NEGATIVE':\n",
    "            headlines_df.at[idx, 'headline_sentiment'] = -1\n",
    "\n",
    "        ## analyse description ##               \n",
    "        result = sentiment_model(description)\n",
    "        label = result[0]['label']\n",
    "\n",
    "        if label == 'POSITIVE':\n",
    "            headlines_df.at[idx, 'description_sentiment'] = 1\n",
    "        elif label == 'NEGATIVE':\n",
    "            headlines_df.at[idx, 'description_sentiment'] = -1\n",
    "        \n",
    "\n",
    "        ## analyse emotions ##\n",
    "        results = emotion_model(headline)\n",
    "\n",
    "        for result in results:\n",
    "            label = result['label']\n",
    "            if label in valid_emotions:\n",
    "\n",
    "                if label not in emotions:\n",
    "                    emotions[label] = 0\n",
    "\n",
    "                emotions[label] += 1\n",
    "\n",
    "                headlines_df.at[idx, \"emotion\"] = label\n",
    "\n",
    "        ## analyse keywords ##\n",
    "        results = extract_keywords(headline)\n",
    "\n",
    "\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'battery': 1, 'trillion': 3, 'game': 2, 'show': 2, 'Donald': 1, 'Trump': 1, 'Joe': 1, 'administration': 1, 'tax': 1, 'electric': 1, 'vehicle': 1, 'McDonald': 1, 'Coca': 1, 'Cola': 1, 'Bernard': 1, 'Jeff': 1, 'Larry': 1, 'Ellison': 1, 'Ware': 1}\n"
     ]
    }
   ],
   "source": [
    "batch_headlines = \" \".join(headlines) + \" \".join(descriptions)\n",
    "\n",
    "extract_keywords(batch_headlines)\n",
    "\n",
    "print(keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headline Sentiment Score: -3\n",
      "Description Sentiment Score: -1\n"
     ]
    }
   ],
   "source": [
    "headlines_df.head()\n",
    "\n",
    "headlines_score = headlines_df['headline_sentiment'].sum()\n",
    "description_score = headlines_df['description_sentiment'].sum()\n",
    "\n",
    "print(f\"Headline Sentiment Score: {headlines_score}\")\n",
    "print(f\"Description Sentiment Score: {description_score}\")\n",
    "\n",
    "# print(emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joy': 0, 'others': 4, 'surprise': 0, 'sadness': 0, 'fear': 0, 'anger': 1, 'disgust': 0}\n"
     ]
    }
   ],
   "source": [
    "print(emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1, -1, -1, 1, -1]\n",
      "-3\n",
      "========================\n",
      "[-1, -1, -1, 1, 1]\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "def normalise_sentiment(lst):\n",
    "    # sum of all values\n",
    "    total = sum(lst)\n",
    "\n",
    "    return total\n",
    "    \n",
    "\n",
    "senti1,senti2 = [headlines_df['headline_sentiment'].tolist(), headlines_df['description_sentiment'].tolist()]\n",
    "\n",
    "print(senti1)\n",
    "print(normalise_sentiment(senti1))\n",
    "\n",
    "print(\"========================\")\n",
    "\n",
    "print(senti2)\n",
    "print(normalise_sentiment(senti2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some code to display emotions in a graph\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# print(emotions.keys())\n",
    "\n",
    "emotionList = list(emotions.keys())\n",
    "valuesList = list(emotions.values())\n",
    "\n",
    "print(emotionList)\n",
    "print(valuesList)\n",
    "\n",
    "ax.bar(emotionList, valuesList)\n",
    "\n",
    "# show count of each emotion\n",
    "for i in range(len(emotionList)):\n",
    "    ax.text(i, valuesList[i], str(valuesList[i]), ha='center')\n",
    "\n",
    "# different colors for each bar\n",
    "ax.bar(emotionList, valuesList, color=['red', 'green', 'blue', 'purple', 'orange', 'black', 'pink'])\n",
    "\n",
    "# label \"others\" as \"neutral\"\n",
    "ax.set_xticklabels(['joy', 'neutral', 'surprise', 'sadness', 'fear', 'anger', 'disgust'])\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## py mongo connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully connected\n",
      "InsertOneResult(ObjectId('65daac0456cf6fddcff11673'), acknowledged=True)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# uri = \"mongodb+srv://admin:admin@sentimentcluster.1cmwzaj.mongodb.net/?retryWrites=true&w=majority\"\n",
    "\n",
    "db = None\n",
    "collection = None\n",
    "\n",
    "\n",
    "def db_init():\n",
    "    uri = \"mongodb://localhost:27017/\"\n",
    "\n",
    "    # myclient = pymongo.MongoClient(uri)\n",
    "\n",
    "    try:\n",
    "      print(\"successfully connected\")\n",
    "      client = pymongo.MongoClient(uri)\n",
    "      \n",
    "    # return a friendly error if a URI error is thrown \n",
    "    except pymongo.errors.ConfigurationError:\n",
    "      print(\"An Invalid URI host error was received. Is your Atlas host name correct in your connection string?\")\n",
    "      sys.exit(1)\n",
    "\n",
    "\n",
    "    db = client.SentimentCluster\n",
    "\n",
    "    collection = db[\"sentiment_data\"]\n",
    "\n",
    "    try:\n",
    "      collection.drop()  \n",
    "      return collection\n",
    "\n",
    "    # return a friendly error if an authentication error is thrown\n",
    "    except pymongo.errors.OperationFailure:\n",
    "      print(\"An authentication error was received. Are your username and password correct in your connection string?\")\n",
    "      sys.exit(1)\n",
    "\n",
    "def db_insert():\n",
    "    try: \n",
    "    #  result = collection.insert_one({\"search\":\"tesla\", \"keywords\":keywords})\n",
    "      result = collection.insert_one({\n",
    "              \"datetime\" : datetime.now(),\n",
    "              \"search\" : \"tesla\",\n",
    "              'sentiment_score' : str(headlines_score + description_score),\n",
    "              'emotion' : emotions,\n",
    "              'keyword' : keywords\n",
    "              })\n",
    "\n",
    "    # return a friendly error if the operation fails\n",
    "    except pymongo.errors.OperationFailure:\n",
    "      print(\"An authentication error was received. Are you sure your database user is authorized to perform write operations?\")\n",
    "      sys.exit(1)\n",
    "    else:\n",
    "    # inserted_count = len(result.inserted_ids)\n",
    "    # print(\"I inserted %x documents.\" %(inserted_count))\n",
    "      print(result)\n",
    "\n",
    "      print(\"\\n\")\n",
    "\n",
    "\n",
    "collection = db_init()\n",
    "db_insert()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('65daa8c956cf6fddcff1166e'), 'datetime': datetime.datetime(2024, 2, 25, 10, 41, 13, 851000), 'search': 'test', 'sentiment_score': '-4', 'emotion': {'joy': 0, 'others': 4, 'surprise': 0, 'sadness': 0, 'fear': 0, 'anger': 1, 'disgust': 0}, 'keyword': {'battery': 1, 'trillion': 3, 'game': 2, 'show': 2, 'Donald': 1, 'Trump': 1, 'Joe': 1, 'administration': 1, 'tax': 1, 'electric': 1, 'vehicle': 1, 'McDonald': 1, 'Coca': 1, 'Cola': 1, 'Bernard': 1, 'Jeff': 1, 'Larry': 1, 'Ellison': 1, 'Ware': 1}}\n",
      "Data is fresh\n"
     ]
    }
   ],
   "source": [
    "my_doc = collection.find_one({\"search\": \"tesla\"})\n",
    "\n",
    "print(my_doc)\n",
    "\n",
    "\n",
    "# check if datetime is older thatn 30 mins ago\n",
    "if (datetime.now() - my_doc['datetime']).seconds > 1800:\n",
    "    print(\"Data is older than 30 mins\")\n",
    "\n",
    "    # do sentiment analysis and emotions analysis\n",
    "\n",
    "    # find and replace the document\n",
    "    collection.replace_one({\"search\": \"tesla\"}, {\n",
    "        \"datetime\": datetime.now(),\n",
    "        \"search\": \"tesla\",\n",
    "        'sentiment_score': str(headlines_score + description_score),\n",
    "        'emotion': emotions,\n",
    "        'keyword': keywords\n",
    "    })\n",
    "\n",
    "    \n",
    "else:\n",
    "    print(\"Data is fresh\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIND DOCUMENTS\n",
    "#\n",
    "# Now that we have data in Atlas, we can read it. To retrieve all of\n",
    "# the data in a collection, we call find() with an empty filter. \n",
    "\n",
    "result = my_collection.find()\n",
    "\n",
    "if result:    \n",
    "  for doc in result:\n",
    "    my_recipe = doc['name']\n",
    "    my_ingredient_count = len(doc['ingredients'])\n",
    "    my_prep_time = doc['prep_time']\n",
    "    print(\"%s has %x ingredients and takes %x minutes to make.\" %(my_recipe, my_ingredient_count, my_prep_time))\n",
    "    \n",
    "else:\n",
    "  print(\"No documents found.\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# We can also find a single document. Let's find a document\n",
    "# that has the string \"potato\" in the ingredients list.\n",
    "my_doc = my_collection.find_one({\"ingredients\": \"potato\"})\n",
    "\n",
    "if my_doc is not None:\n",
    "  print(\"A recipe which uses potato:\")\n",
    "  print(my_doc)\n",
    "else:\n",
    "  print(\"I didn't find any recipes that contain 'potato' as an ingredient.\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# UPDATE A DOCUMENT\n",
    "#\n",
    "# You can update a single document or multiple documents in a single call.\n",
    "# \n",
    "# Here we update the prep_time value on the document we just found.\n",
    "#\n",
    "# Note the 'new=True' option: if omitted, find_one_and_update returns the\n",
    "# original document instead of the updated one.\n",
    "\n",
    "my_doc = my_collection.find_one_and_update({\"ingredients\": \"potato\"}, {\"$set\": { \"prep_time\": 72 }}, new=True)\n",
    "if my_doc is not None:\n",
    "  print(\"Here's the updated recipe:\")\n",
    "  print(my_doc)\n",
    "else:\n",
    "  print(\"I didn't find any recipes that contain 'potato' as an ingredient.\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# DELETE DOCUMENTS\n",
    "#\n",
    "# As with other CRUD methods, you can delete a single document \n",
    "# or all documents that match a specified filter. To delete all \n",
    "# of the documents in a collection, pass an empty filter to \n",
    "# the delete_many() method. In this example, we'll delete two of \n",
    "# the recipes.\n",
    "#\n",
    "# The query filter passed to delete_many uses $or to look for documents\n",
    "# in which the \"name\" field is either \"elotes\" or \"fried rice\".\n",
    "\n",
    "my_result = my_collection.delete_many({ \"$or\": [{ \"name\": \"elotes\" }, { \"name\": \"fried rice\" }]})\n",
    "print(\"I deleted %x records.\" %(my_result.deleted_count))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pymongo docker\n",
    "\n",
    "- https://www.youtube.com/watch?v=RuaKvPq0Fzo\n",
    "- https://medium.com/analytics-vidhya/creating-dockerized-flask-mongodb-application-20ccde391a\n",
    "\n",
    "\n",
    "to run one mongo docker image: docker run -d -p 27017:27017 --name m1 mongo\n",
    "\n",
    "then can use: mongodb://localhost:27017/ as uri\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
